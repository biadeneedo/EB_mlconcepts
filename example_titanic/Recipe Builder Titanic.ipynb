{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recipe Builder Actions Overview\n",
    "\n",
    "### Saving a File Cell\n",
    "If you wish to save the contents of a cell, simply run it.  The `%%writefile` command at the top of the cell will write the contents of the cell to the file named at the top of the cell. You should run the cells manually when applicable. However, **pressing any of the actions at the top will automatically run all file cells relevant to the action**.\n",
    "\n",
    "### Training and Scoring\n",
    "Press the associated buttons at the top in order to run training or scoring. The training output will be shown below the `evaluator.py` cell and scoring output will be shown below the `datasaver.py` cell. You must run training at least once before you can run scoring. You may delete the output cell(s). Running training the first time or after changing `requirements.txt` will be slower since the dependencies for the recipe need to be installed, but subsequent runs will be significantly faster. If you wish to see the hidden output add `debug` to the end of the output cell and re-run it.\n",
    "\n",
    "### Creating the Recipe\n",
    "When you are done editing the recipe and satisfied with the training/scoring output, you can create a recipe from the notebook by pressing `Create Recipe`. You must run scoring at least once before you can create the recipe. After pressing it, you will see a progress bar showing how much time is left for the build to finish. If the recipe creation is successful the progress bar will be replaced by an external link that you can click to navigate to the created recipe.\n",
    "\n",
    "\n",
    "## Caution!\n",
    "* **Do not delete any of the file cells**\n",
    "* **Do not edit the `%%writefile` line at the top of the file cells**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Requirements File** (Optional)\n",
    "Add additional libraries you wish to use in the recipe to the cell below. You can specify the version number if necessary. The file cell below is a **commented out example**.  The file structure is yaml. It adheres to the specification outlined in the [Conda Documentation](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-file-manually). **NOTE:** The name field is not allowed to be overridden in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "requirements.txt",
    "tags": [
     "requirements.txt"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/requirements.txt\n",
    "\n",
    "#dependencies:\n",
    "## Conda dependencies should be attempted first\n",
    "#- statsmodels=0.10.2\n",
    "#- pip\n",
    "#- pip:\n",
    "## Pip installs can be listed next but should only be used when Conda is unavailable\n",
    "#   - pmdarima==1.5.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search here for additional libraries https://anaconda.org/. This is the list of main **libraries already in use**:\n",
    "`python=3.6.7` `scikit-learn` `pandas` `numpy` `data_access_sdk_python`\n",
    "**Warning: libraries or specific versions you add may be incompatible with the above libraries**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Configuration Files**\n",
    "List any hyperparameters you wish to use. Specify the dataset(s) and schema(s) that are needed for training/scoring. To find the dataset ids go to the **Data tab** in Adobe Experience Platform or view the **Datasets** folder in the **Notebooks Data tab** on the left. You can also find schema id in the **Notebooks Data tab** under the **Schemas** folder. Each configuration will only be used for its corresponding action. `ACP_DSW_TRAINING_XDM_SCHEMA` and `ACP_DSW_SCORING_RESULTS_XDM_SCHEMA` will only be used after the recipe has been created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "training.conf",
    "tags": [
     "training.conf"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/training.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/training.conf\n",
    "\n",
    "{\n",
    "   \"trainingDataSetId\": \"6144b1cba3485119497a6f86\",\n",
    "   \"criterion\": \"gini\",\n",
    "   \"max_depth\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scoring Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "scoring.conf",
    "tags": [
     "scoring.conf"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/scoring.conf\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/scoring.conf\n",
    "\n",
    "{\n",
    "   \"scoringDataSetId\": \"6144b224fd59b719489bf07b\",\n",
    "   \"scoringResultsDataSetId\": \"6148c1002e60e11949ac2ea3\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following configuration parameters are automatically set for you when you train/score:** \n",
    "`ML_FRAMEWORK_IMS_USER_CLIENT_ID` `ML_FRAMEWORK_IMS_TOKEN` `ML_FRAMEWORK_IMS_ML_TOKEN` `ML_FRAMEWORK_IMS_TENANT_ID`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training Data Loader File**\n",
    "Implement the `load` function to load and prepare the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "trainingdataloader.py",
    "tags": [
     "trainingdataloader.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/trainingdataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/trainingdataloader.py\n",
    "\n",
    "# LIBRARIES\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np  \n",
    "import pandas as pd      \n",
    "from platform_sdk.dataset_reader import DatasetReader\n",
    "from .utils import get_client_context\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "# FUNZIONE DI TRAINING\n",
    "def load(config_properties):\n",
    "    print(\"Training Data Load Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Load Data\n",
    "    #########################################    \n",
    "    client_context = get_client_context(config_properties)\n",
    "    dataset_reader = DatasetReader(client_context, config_properties['trainingDataSetId'])\n",
    "    titanic = dataset_reader.limit(10000000).read()\n",
    "        \n",
    "    #########################################\n",
    "    # Data Preprocessing\n",
    "    #########################################    \n",
    "    titanic['Title'] = titanic.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "    new_titles = {\n",
    "        \"Capt\":       \"Officer\",\n",
    "        \"Col\":        \"Officer\",\n",
    "        \"Major\":      \"Officer\",\n",
    "        \"Jonkheer\":   \"Royalty\",\n",
    "        \"Don\":        \"Royalty\",\n",
    "        \"Sir\" :       \"Royalty\",\n",
    "        \"Dr\":         \"Officer\",\n",
    "        \"Rev\":        \"Officer\",\n",
    "        \"the Countess\":\"Royalty\",\n",
    "        \"Dona\":       \"Royalty\",\n",
    "        \"Mme\":        \"Mrs\",\n",
    "        \"Mlle\":       \"Miss\",\n",
    "        \"Ms\":         \"Mrs\",\n",
    "        \"Mr\" :        \"Mr\",\n",
    "        \"Mrs\" :       \"Mrs\",\n",
    "        \"Miss\" :      \"Miss\",\n",
    "        \"Master\" :    \"Master\",\n",
    "        \"Lady\" :      \"Royalty\"\n",
    "    }\n",
    "\n",
    "    titanic.Title = titanic.Title.map(new_titles)\n",
    "\n",
    "    titanic.Cabin = titanic.Cabin.fillna(0)\n",
    "    for i in range(len(titanic.Cabin)): \n",
    "        if titanic.Cabin[i] != 0:\n",
    "            titanic.Cabin[i] = 1\n",
    "\n",
    "    grouped = titanic.groupby(['Sex','Pclass', 'Title'])\n",
    "    titanic.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    most_embarked = titanic.Embarked.value_counts().index[0]\n",
    "    titanic.Embarked = titanic.Embarked.fillna(most_embarked)\n",
    "    titanic.Fare = titanic.Fare.fillna(titanic.Fare.median())\n",
    "\n",
    "    titanic.drop('Name', axis =1, inplace=True)\n",
    "    titanic.drop('Ticket', axis =1, inplace=True)\n",
    "    titanic.drop('PassengerId', axis=1, inplace = True)\n",
    "    \n",
    "    titanic['SibSp'] = titanic['SibSp'].apply(lambda x: int(x))\n",
    "    titanic['Parch'] = titanic['Parch'].apply(lambda x: int(x))\n",
    "    \n",
    "    Sex = {\"male\": 0, \"female\":1}\n",
    "    titanic[\"Sex\"] = titanic.Sex.map(Sex)\n",
    "    titanic['Partner'] = titanic['SibSp'] + titanic['Parch'] # \n",
    "    titanic.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "    titanic = pd.get_dummies(titanic, columns = ['Title','Embarked'])\n",
    "    \n",
    "    #########################################\n",
    "    # Return Dataset\n",
    "    ######################################### \n",
    "    print('Return Dataset')\n",
    "    print(titanic.shape)\n",
    "    print(titanic.head())\n",
    "    print(\"Training Data Load Finish\")\n",
    "    return titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scoring Data Loader File**\n",
    "Implement the `load` function to load and prepare the scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "scoringdataloader.py",
    "tags": [
     "scoringdataloader.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/scoringdataloader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/scoringdataloader.py\n",
    "\n",
    "# LIBRARIES\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np  \n",
    "import pandas as pd    \n",
    "from platform_sdk.dataset_reader import DatasetReader\n",
    "from .utils import get_client_context\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "\n",
    "def load(config_properties):\n",
    "\n",
    "    print(\"Scoring Data Load Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Load Data\n",
    "    #########################################    \n",
    "    client_context = get_client_context(config_properties)\n",
    "    dataset_reader = DatasetReader(client_context, config_properties['scoringDataSetId'])\n",
    "    titanic = dataset_reader.limit(10000000).read()\n",
    "    \n",
    "    #########################################\n",
    "    # Data Preprocessing\n",
    "    #########################################    \n",
    "    titanic['Title'] = titanic.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())\n",
    "    new_titles = {\n",
    "        \"Capt\":       \"Officer\",\n",
    "        \"Col\":        \"Officer\",\n",
    "        \"Major\":      \"Officer\",\n",
    "        \"Jonkheer\":   \"Royalty\",\n",
    "        \"Don\":        \"Royalty\",\n",
    "        \"Sir\" :       \"Royalty\",\n",
    "        \"Dr\":         \"Officer\",\n",
    "        \"Rev\":        \"Officer\",\n",
    "        \"the Countess\":\"Royalty\",\n",
    "        \"Dona\":       \"Royalty\",\n",
    "        \"Mme\":        \"Mrs\",\n",
    "        \"Mlle\":       \"Miss\",\n",
    "        \"Ms\":         \"Mrs\",\n",
    "        \"Mr\" :        \"Mr\",\n",
    "        \"Mrs\" :       \"Mrs\",\n",
    "        \"Miss\" :      \"Miss\",\n",
    "        \"Master\" :    \"Master\",\n",
    "        \"Lady\" :      \"Royalty\"\n",
    "    }\n",
    "\n",
    "    titanic.Title = titanic.Title.map(new_titles)\n",
    "\n",
    "    titanic.Cabin = titanic.Cabin.fillna(0)\n",
    "    for i in range(len(titanic.Cabin)): \n",
    "        if titanic.Cabin[i] != 0:\n",
    "            titanic.Cabin[i] = 1\n",
    "\n",
    "    grouped = titanic.groupby(['Sex','Pclass', 'Title'])\n",
    "    titanic.Age = grouped.Age.apply(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    most_embarked = titanic.Embarked.value_counts().index[0]\n",
    "    titanic.Embarked = titanic.Embarked.fillna(most_embarked)\n",
    "    titanic.Fare = titanic.Fare.fillna(titanic.Fare.median())\n",
    "\n",
    "    titanic.drop('Name', axis =1, inplace=True)\n",
    "    titanic.drop('Ticket', axis =1, inplace=True)\n",
    "    titanic.drop('PassengerId', axis=1, inplace = True)\n",
    "    \n",
    "    titanic['SibSp'] = titanic['SibSp'].apply(lambda x: int(x))\n",
    "    titanic['Parch'] = titanic['Parch'].apply(lambda x: int(x))\n",
    "    \n",
    "    Sex = {\"male\": 0, \"female\":1}\n",
    "    titanic[\"Sex\"] = titanic.Sex.map(Sex)\n",
    "    titanic['Partner'] = titanic['SibSp'] + titanic['Parch'] # \n",
    "    titanic.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "    titanic = pd.get_dummies(titanic, columns = ['Title','Embarked'])\n",
    "    \n",
    "    \n",
    "    #########################################\n",
    "    # Return Dataset\n",
    "    #########################################\n",
    "    print('Return Dataset')\n",
    "    print(titanic.shape)\n",
    "    print(titanic.head())\n",
    "    print(\"Scoring Data Load Finish\")\n",
    "    return titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pipeline File**\n",
    "Implement the `train` function and return the trained model. Implement the `score` function to return a prediction made on the scoring data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pipeline.py",
    "tags": [
     "pipeline.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/pipeline.py\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, TimeSeriesSplit, RepeatedKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "\n",
    "def train(config_properties, data):\n",
    "\n",
    "    print(\"Train Start\")\n",
    "\n",
    "    #########################################\n",
    "    # Define & Fit model\n",
    "    #########################################\n",
    "    y_train = data.Survived\n",
    "    X_train = data.drop(columns = ['Survived'])\n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    model = DecisionTreeClassifier(criterion=config_properties['criterion'],\n",
    "                                    max_depth=config_properties['max_depth'])\n",
    "    model = model.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Train Complete\")\n",
    "    return model\n",
    "\n",
    "def score(config_properties, data, model):\n",
    "\n",
    "    print(\"Score Start\")\n",
    "    y_pred = model.predict(data)\n",
    "    data['prediction'] = y_pred\n",
    "    data = data.reset_index()\n",
    "\n",
    "    print(\"Score Complete\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluator File**\n",
    "Implement the `split` function to partition the training data and the `evaluate` function to the return the validation metrics you wish to see. Training output will be shown below this file cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "evaluator.py",
    "tags": [
     "evaluator.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/evaluator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/evaluator.py\n",
    "\n",
    "from ml.runtime.python.core.regressionEvaluator import RegressionEvaluator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Evaluator(RegressionEvaluator):\n",
    "    def __init__(self):\n",
    "        print (\"Initiate Evaluation\")\n",
    "\n",
    "    def split(self, config={}, dataframe=None):\n",
    "        df_train, df_val = train_test_split(dataframe, test_size=0.20, random_state=16)\n",
    "        return df_train, df_val\n",
    "\n",
    "    def evaluate(self, data=[], model={}, config={}):\n",
    "        print (\"Evaluation evaluate triggered\")\n",
    "        val = data.drop('Survived', axis=1)\n",
    "        y_pred = model.predict(val)\n",
    "        y_actual = data['Survived'].values\n",
    "        print(type(y_pred))\n",
    "        print(type(y_actual))\n",
    "        acc = accuracy_score(y_actual, y_pred)\n",
    "        prec = precision_score(y_actual, y_pred, pos_label='1')\n",
    "        metric = [{\"name\": \"ACC\", \"value\": acc, \"valueType\": \"double\"},\n",
    "                 {\"name\": \"PREC\", \"value\": prec, \"valueType\": \"double\"}]\n",
    "        \n",
    "        print(metric)\n",
    "        return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "training-cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING TRAINING...\n",
      "INSTALLING DEPENDENCIES...\n",
      "Ignore the exception for python runtime cannot import name 'BlockBlobService' from 'azure.storage.blob' (/opt/conda/envs/usermlruntimepython/lib/python3.7/site-packages/azure/storage/blob/__init__.py)\n",
      "2021-09-21 08:59:28,756 INFO trainingInitiator.py: Trainer initiated\n",
      "INFO:trainingInitiator.py:Trainer initiated\n",
      "2021-09-21 08:59:28,767 INFO trainingInitiator.py: Evaluator initiated\n",
      "INFO:trainingInitiator.py:Evaluator initiated\n",
      "2021-09-21 08:59:28,768 INFO main.py: Training starts, testing:True, conf:/tmp/tmp.EqQEpT5PxZ/token.conf\n",
      "INFO:main.py:Training starts, testing:True, conf:/tmp/tmp.EqQEpT5PxZ/token.conf\n",
      "2021-09-21 08:59:28,769 INFO trainingInitiator.py: Training class is not of type Tensorflow\n",
      "INFO:trainingInitiator.py:Training class is not of type Tensorflow\n",
      "2021-09-21 08:59:28,769 INFO trainingInitiator.py: Python Job\n",
      "INFO:trainingInitiator.py:Python Job\n",
      "2021-09-21 08:59:28,769 INFO trainingInitiator.py: Load the dataframe\n",
      "INFO:trainingInitiator.py:Load the dataframe\n",
      "Training Data Load Start\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to get dataset details from catalog and make PQS connection: 1.69\n",
      "INFO:PlatformSDKPython:dataset_id: 6144b1cba3485119497a6f86, limit: 10000000\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to execute query: 11.97\n",
      "INFO:PlatformSDKPython:dataset_reader: 891 rows read. 90.62 MB memory used for this process\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to format data of dataframe: 0.02\n",
      "Return Dataset\n",
      "(891, 16)\n",
      "  Survived Pclass  Sex Age  ... Title_Royalty Embarked_C Embarked_Q  Embarked_S\n",
      "0        0      3    0  22  ...             0          0          0           1\n",
      "1        1      1    1  38  ...             0          1          0           0\n",
      "2        1      3    1  26  ...             0          0          0           1\n",
      "3        1      1    1  35  ...             0          0          0           1\n",
      "4        0      3    0  35  ...             0          0          0           1\n",
      "\n",
      "[5 rows x 16 columns]\n",
      "Training Data Load Finish\n",
      "2021-09-21 08:59:42,602 INFO trainingInitiator.py: Evaluator class is defined for python\n",
      "INFO:trainingInitiator.py:Evaluator class is defined for python\n",
      "Initiate Evaluation\n",
      "2021-09-21 08:59:42,604 INFO trainingInitiator.py: Python Training started\n",
      "INFO:trainingInitiator.py:Python Training started\n",
      "Train Start\n",
      "(712, 15)\n",
      "(712,)\n",
      "Train Complete\n",
      "2021-09-21 08:59:42,706 INFO Helper.py: Save Model completed : /home/asruser/my-workspace/.recipes/trainedModels/recipe-Ru_oljS6g/trainedModel\n",
      "INFO:Helper.py:Save Model completed : /home/asruser/my-workspace/.recipes/trainedModels/recipe-Ru_oljS6g/trainedModel\n",
      "2021-09-21 08:59:42,707 INFO trainingInitiator.py: Python Training completed\n",
      "INFO:trainingInitiator.py:Python Training completed\n",
      "2021-09-21 08:59:42,707 INFO trainingInitiator.py: Evaluation will be on the test data\n",
      "INFO:trainingInitiator.py:Evaluation will be on the test data\n",
      "2021-09-21 08:59:42,707 INFO trainingInitiator.py: Evaluate config is set to true\n",
      "INFO:trainingInitiator.py:Evaluate config is set to true\n",
      "2021-09-21 08:59:42,707 INFO evaluateInitiator.py: Starting evaluation\n",
      "INFO:evaluateInitiator.py:Starting evaluation\n",
      "Initiate Evaluation\n",
      "Evaluation evaluate triggered\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "[{'name': 'ACC', 'value': 0.7988826815642458, 'valueType': 'double'}, {'name': 'PREC', 'value': 0.78125, 'valueType': 'double'}]\n",
      "2021-09-21 08:59:42,713 INFO evaluateInitiator.py: Evaluation completed\n",
      "INFO:evaluateInitiator.py:Evaluation completed\n",
      "TRAINING SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "!bash -e run_action.sh recipe-Ru_oljS6g training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Saver File**\n",
    "Implement the `save` function for saving your prediction. Scoring output will be added below this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "datasaver.py",
    "tags": [
     "datasaver.py"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/asruser/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/datasaver.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ~/my-workspace/.recipes/recipe-Ru_oljS6g/recipe/datasaver.py\n",
    "\n",
    "import pandas as pd\n",
    "from .utils import get_client_context\n",
    "from platform_sdk.models import Dataset\n",
    "from platform_sdk.dataset_writer import DatasetWriter\n",
    "from datetime import date\n",
    "\n",
    "def save(config_properties, prediction):\n",
    "    print(\"Datasaver Start\")\n",
    "    print(prediction)\n",
    "    prediction['timestamp'] = pd.to_datetime(date.today())\n",
    "    prediction = prediction[['prediction', 'Sex', 'timestamp']]\n",
    "\n",
    "    prediction.rename({'prediction':'output.PREDICTION',\n",
    "                     'Sex':'output.SEX',\n",
    "                     'timestamp':'output.TIMESTAMP'\n",
    "                    },\n",
    "                      inplace = True, axis = 1)\n",
    "\n",
    "    client_context = get_client_context(config_properties)\n",
    "    tenant_id = config_properties.get(\"tenantId\")\n",
    "    prediction = prediction.add_prefix(tenant_id+\".\")\n",
    "    print(prediction.columns)\n",
    "    print('prediction after renaming: ', prediction.head(5))\n",
    "    dataset = Dataset(client_context).get_by_id(config_properties['scoringResultsDataSetId'])\n",
    "    print(dataset)\n",
    "    dataset_writer = DatasetWriter(client_context, dataset)\n",
    "    dataset_writer.write(prediction, file_format='json')\n",
    "\n",
    "    print(\"Datasaver Finish\")\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "scoring-cell"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING SCORING...\n",
      "Ignore the exception for python runtime cannot import name 'BlockBlobService' from 'azure.storage.blob' (/opt/conda/envs/usermlruntimepython/lib/python3.7/site-packages/azure/storage/blob/__init__.py)\n",
      "2021-09-21 09:03:53,262 INFO main.py: Scoring starts, testing:True, conf:/tmp/tmp.WWQaLyPfZl/token.conf\n",
      "2021-09-21 09:03:56,368 INFO scoringInitiator.py: Scorer initiated\n",
      "INFO:scoringInitiator.py:Scorer initiated\n",
      "2021-09-21 09:03:56,372 INFO scoringInitiator.py: Scoring class is not of type Tensorflow\n",
      "INFO:scoringInitiator.py:Scoring class is not of type Tensorflow\n",
      "2021-09-21 09:03:56,372 INFO scoringInitiator.py: Python scoring starts\n",
      "INFO:scoringInitiator.py:Python scoring starts\n",
      "2021-09-21 09:03:56,373 INFO scoringInitiator.py: Python executed scoring\n",
      "INFO:scoringInitiator.py:Python executed scoring\n",
      "Scoring Data Load Start\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to get dataset details from catalog and make PQS connection: 1.17\n",
      "INFO:PlatformSDKPython:dataset_id: 6144b224fd59b719489bf07b, limit: 10000000\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to execute query: 25.68\n",
      "INFO:PlatformSDKPython:dataset_reader: 418 rows read. 90.05 MB memory used for this process\n",
      "INFO:PlatformSDKPython:dataset_reader: seconds taken to format data of dataframe: 0.02\n",
      "Return Dataset\n",
      "(418, 15)\n",
      "  Pclass  Sex   Age     Fare  ... Title_Royalty Embarked_C  Embarked_Q  Embarked_S\n",
      "0      3    0  34.5   7.8292  ...             0          0           1           0\n",
      "1      3    1    47        7  ...             0          0           0           1\n",
      "2      2    0    62   9.6875  ...             0          0           1           0\n",
      "3      3    0    27   8.6625  ...             0          0           0           1\n",
      "4      3    1    22  12.2875  ...             0          0           0           1\n",
      "\n",
      "[5 rows x 15 columns]\n",
      "Scoring Data Load Finish\n",
      "Score Start\n",
      "Score Complete\n",
      "Datasaver Start\n",
      "     index Pclass  Sex   Age  ... Embarked_C Embarked_Q Embarked_S  prediction\n",
      "0        0      3    0  34.5  ...          0          1          0           0\n",
      "1        1      3    1    47  ...          0          0          1           1\n",
      "2        2      2    0    62  ...          0          1          0           0\n",
      "3        3      3    0    27  ...          0          0          1           0\n",
      "4        4      3    1    22  ...          0          0          1           1\n",
      "..     ...    ...  ...   ...  ...        ...        ...        ...         ...\n",
      "413    413      3    0  25.0  ...          0          0          1           0\n",
      "414    414      1    1    39  ...          1          0          0           1\n",
      "415    415      3    0  38.5  ...          0          0          1           0\n",
      "416    416      3    0  25.0  ...          0          0          1           0\n",
      "417    417      3    0   7.0  ...          1          0          0           1\n",
      "\n",
      "[418 rows x 17 columns]\n",
      "Index(['_deloitteemeanorthpartnersand.output.PREDICTION',\n",
      "       '_deloitteemeanorthpartnersand.output.SEX',\n",
      "       '_deloitteemeanorthpartnersand.output.TIMESTAMP'],\n",
      "      dtype='object')\n",
      "prediction after renaming:    _deloitteemeanorthpartnersand.output.PREDICTION  ...  _deloitteemeanorthpartnersand.output.TIMESTAMP\n",
      "0                                               0  ...                                      2021-09-21\n",
      "1                                               1  ...                                      2021-09-21\n",
      "2                                               0  ...                                      2021-09-21\n",
      "3                                               0  ...                                      2021-09-21\n",
      "4                                               1  ...                                      2021-09-21\n",
      "\n",
      "[5 rows x 3 columns]\n",
      "<platform_sdk.models.Dataset object at 0x7fdb5a9018d0>\n",
      "INFO:azure.datalake.store.core:closing stream\n",
      "INFO:azure.datalake.store.transfer:Transferred tempFile.parquet -> /foundation/data/stage/users/76115D0C5FF87D8B0A495E0B@AdobeID/01FG3SJQ838XVF2AX4Z3EKXKZK/6148c1012e60e11949ac2ea4/1632215065177.json\n",
      "INFO:PlatformSDKPython:dataset_writer: 418 rows written. 94.21 MB memory used for this process\n",
      "Datasaver Finish\n",
      "    _deloitteemeanorthpartnersand.output.PREDICTION  ...  _deloitteemeanorthpartnersand.output.TIMESTAMP\n",
      "0                                                 0  ...                             2021-09-21T00:00:00\n",
      "1                                                 1  ...                             2021-09-21T00:00:00\n",
      "2                                                 0  ...                             2021-09-21T00:00:00\n",
      "3                                                 0  ...                             2021-09-21T00:00:00\n",
      "4                                                 1  ...                             2021-09-21T00:00:00\n",
      "..                                              ...  ...                                             ...\n",
      "413                                               0  ...                             2021-09-21T00:00:00\n",
      "414                                               1  ...                             2021-09-21T00:00:00\n",
      "415                                               0  ...                             2021-09-21T00:00:00\n",
      "416                                               0  ...                             2021-09-21T00:00:00\n",
      "417                                               1  ...                             2021-09-21T00:00:00\n",
      "\n",
      "[418 rows x 3 columns]\n",
      "2021-09-21 09:04:27,023 INFO scoringInitiator.py: Python scoring completed\n",
      "INFO:scoringInitiator.py:Python scoring completed\n",
      "SCORING SUCCESSFUL!\n"
     ]
    }
   ],
   "source": [
    "!bash -e run_action.sh recipe-Ru_oljS6g scoring"
   ]
  }
 ],
 "metadata": {
  "elementId": "grYBrq-eT",
  "engine_name": "titanic",
  "isScoringRun": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "last_created_recipe": "https://platform.adobe.com/ml/recipes/7fb70d8d-76c0-4e2b-aa94-13cc8e76de9a",
  "notebook_type": "builder",
  "recipe_name": "recipe-Ru_oljS6g"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
